import os
import asyncio
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor
from sentence_transformers import SentenceTransformer
from openai import AsyncOpenAI
from dotenv import load_dotenv
from app.common.logging.logging_config import get_logger

load_dotenv()

logger = get_logger(__name__)


class EmbeddingModel:
    def __init__(self, model_name: str = "jhgan/ko-sroberta-multitask"):
        """
        ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”

        Args:
            model_name: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ëª…
                - OpenAI: "text-embedding-3-small", "text-embedding-3-large"
                - Local: "jhgan/ko-sroberta-multitask" (í•œêµ­ì–´ ì „ìš©), "sentence-transformers/all-MiniLM-L6-v2"
        """
        self.model_name = model_name
        self.openai_api_key = os.getenv("OPENAI_API_KEY")

        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë°°ì¹˜ í¬ê¸°ì™€ ì›Œì»¤ ìˆ˜ ì„¤ì •
        self.batch_size = int(os.getenv("EMBEDDING_BATCH_SIZE", "50"))
        self.max_workers = int(os.getenv("MAX_WORKERS", "4"))

        # ThreadPoolExecutor ì´ˆê¸°í™”
        self.executor = ThreadPoolExecutor(max_workers=self.max_workers)

        # OpenAI API í‚¤ê°€ ìˆìœ¼ë©´ OpenAI ì‚¬ìš©, ì—†ìœ¼ë©´ sentence-transformers ì‚¬ìš©
        if self.openai_api_key:
            self.client = AsyncOpenAI(api_key=self.openai_api_key)
            self.use_openai = True
            logger.info("ğŸ”‘ OpenAI ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (ë¹„ë™ê¸°)")
        else:
            self.model = SentenceTransformer(model_name)
            self.use_openai = False
            logger.info(f"ğŸ¤– Sentence Transformers ëª¨ë¸ ì‚¬ìš©: {model_name}")

    async def get_embedding(self, text: str) -> List[float]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤.

        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„°
        """
        embeddings = await self.get_embeddings([text])
        return embeddings[0] if embeddings else []

    async def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì„ë² ë”©í•©ë‹ˆë‹¤.

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        if not texts:
            return []

        if self.use_openai:
            return await self._get_openai_embeddings_batch(texts)
        else:
            return await self._get_local_embeddings_batch(texts)

    async def _get_openai_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """OpenAI API ë°°ì¹˜ ì²˜ë¦¬"""
        all_embeddings = []

        # í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i + self.batch_size]

            try:
                response = await self.client.embeddings.create(
                    model="text-embedding-ada-002",
                    input=batch_texts
                )
                batch_embeddings = [data.embedding for data in response.data]
                all_embeddings.extend(batch_embeddings)

                # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ (rate limit ë°©ì§€)
                if i + self.batch_size < len(texts):
                    await asyncio.sleep(0.1)

                logger.debug(f"OpenAI ì„ë² ë”© ì§„í–‰ë¥ : {min(i + self.batch_size, len(texts))}/{len(texts)}")

            except Exception as e:
                logger.error(f"OpenAI ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨ (ë°°ì¹˜ {i // self.batch_size + 1}): {e}")
                # í´ë°±ìœ¼ë¡œ ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
                fallback_embeddings = await self._get_local_embeddings_batch(batch_texts)
                all_embeddings.extend(fallback_embeddings)

        return all_embeddings

    async def _get_local_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """ë¡œì»¬ ëª¨ë¸ ë°°ì¹˜ ì²˜ë¦¬ (ThreadPoolExecutor ì‚¬ìš©)"""
        loop = asyncio.get_event_loop()

        async def process_batch(batch_texts):
            return await loop.run_in_executor(
                self.executor,
                lambda: self.model.encode(batch_texts, show_progress_bar=False).tolist()
            )

        all_embeddings = []

        # í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i + self.batch_size]
            batch_embeddings = await process_batch(batch_texts)
            all_embeddings.extend(batch_embeddings)

            logger.debug(f"ë¡œì»¬ ì„ë² ë”© ì§„í–‰ë¥ : {min(i + self.batch_size, len(texts))}/{len(texts)}")

        return all_embeddings

    def prepare_documents_for_indexing(self, data: Dict[str, Any], collection_type: str) -> List[Dict[str, Any]]:
        """
        ë°ì´í„°ë¥¼ ë²¡í„° DB ì¸ë±ì‹±ìš©ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            data: ì›ë³¸ ë°ì´í„°
            collection_type: ì»¬ë ‰ì…˜ íƒ€ì… ('korean_word_problems' ë˜ëŠ” 'card_check')

        Returns:
            ì¸ë±ì‹±ìš© ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        documents = []

        if collection_type == "korean_word_problems":
            # í•œêµ­ì–´ ë‹¨ì–´ ë¬¸ì œ ë°ì´í„° ì²˜ë¦¬
            questions = data.get("questions", [])
            option_cards = data.get("option_cards", [])

            # ë¬¸ì œë³„ë¡œ ë¬¸ì„œ ìƒì„±
            for question in questions:
                # ID í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ID ìƒì„±
                question_id = question.get('id', f"question_{question['number']}")
                doc_text = f"ë¬¸ì œ {question['number']}: {question['sentence']} ì •ë‹µ: {question['answer']}"

                documents.append({
                    "id": question_id,
                    "text": doc_text,
                    "metadata": {
                        "type": "question",
                        "number": str(question['number']),
                        "sentence": question['sentence'],
                        "answer": question['answer'],
                        "collection": collection_type
                    }
                })

            # ì˜µì…˜ ì¹´ë“œë³„ë¡œ ë¬¸ì„œ ìƒì„±
            for i, card in enumerate(option_cards):
                documents.append({
                    "id": f"option_card_{i}",
                    "text": card,
                    "metadata": {
                        "type": "option_card",
                        "card_index": str(i),
                        "content": card,
                        "collection": collection_type
                    }
                })

        elif collection_type == "card_check":
            # ì¹´ë“œ ì²´í¬ ë°ì´í„° ì²˜ë¦¬
            for i, card in enumerate(data):
                doc_text = f"ë‹¨ì–´: {card['word']} ì˜ë¯¸: {card['meaning']}"
                if card.get('examples'):
                    examples_str = ", ".join(card['examples'])
                    doc_text += f" ì˜ˆì‹œ: {examples_str}"

                documents.append({
                    "id": f"card_{i}",
                    "text": doc_text,
                    "metadata": {
                        "type": "card",
                        "word": card['word'],
                        "meaning": card['meaning'],
                        "examples": ", ".join(card.get('examples', [])),
                        "collection": collection_type
                    }
                })

        elif collection_type == "pdf_documents":  # ì‹ ê·œ ì¶”ê°€
            # PDF ë¬¸ì„œ ë°ì´í„°ëŠ” ì´ë¯¸ ì „ì²˜ë¦¬ëœ ìƒíƒœë¡œ ë“¤ì–´ì˜´
            for doc in data:
                documents.append({
                    "id": doc["id"],
                    "text": doc["text"],
                    "metadata": doc["metadata"]
                })

        return documents

    def __del__(self):
        """ì†Œë©¸ìì—ì„œ ThreadPoolExecutor ì •ë¦¬"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=False)


# ì „ì—­ ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
embedding_model = None


def get_embedding_model():
    """ì „ì—­ ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global embedding_model
    if embedding_model is None:
        embedding_model = EmbeddingModel()
    return embedding_model