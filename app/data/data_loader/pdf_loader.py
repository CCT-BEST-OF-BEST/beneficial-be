import os
import logging
from typing import List, Dict, Any
import pdfplumber
import re

logger = logging.getLogger(__name__)

class SimpleTextSplitter:
    """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„í• ê¸° (LangChain ëŒ€ì²´)"""
    
    def __init__(self, chunk_size: int = 800, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = [
            "\n\nì œ", "\n\n",  # ì œNí•­ ìš°ì„ 
            "\nì œ", "ì œ",      # ì œ í‚¤ì›Œë“œ
            "\n\n", "\n",     # ì¼ë°˜ ì¤„ë°”ê¿ˆ
            ".", "!", "?",    # ë¬¸ì¥ ë‹¨ìœ„
            " ", ""           # ìµœí›„ ìˆ˜ë‹¨
        ]
    
    def split_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        current_chunk = ""
        
        # ì¤„ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        lines = text.split('\n')
        
        for line in lines:
            # í˜„ì¬ ì²­í¬ì— ë¼ì¸ì„ ì¶”ê°€í–ˆì„ ë•Œ í¬ê¸° í™•ì¸
            test_chunk = current_chunk + "\n" + line if current_chunk else line
            
            if len(test_chunk) <= self.chunk_size:
                current_chunk = test_chunk
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                # ìƒˆ ì²­í¬ ì‹œì‘ (overlap ê³ ë ¤)
                if len(chunks) > 0 and self.chunk_overlap > 0:
                    overlap_text = current_chunk[-self.chunk_overlap:]
                    current_chunk = overlap_text + "\n" + line
                else:
                    current_chunk = line
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

class PDFDataLoader:
    """PDF ë¬¸ì„œ ë¡œë” ë° ì „ì²˜ë¦¬ê¸° - ì´ˆë“±í•™ìƒ ëŒë´„ë°˜ìš©"""
    
    def __init__(self, chunk_size: int = 800, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = SimpleTextSplitter(chunk_size, chunk_overlap)
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            full_text = ""
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    text = page.extract_text()
                    if text:
                        # í˜ì´ì§€ êµ¬ë¶„ì ì¶”ê°€
                        full_text += f"\n\n[í˜ì´ì§€ {page_num}]\n{text}"
            
            logger.info(f"âœ… PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(full_text)}ì")
            return full_text
            
        except Exception as e:
            logger.error(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    def chunk_pdf_text(self, text: str, pdf_filename: str) -> List[Dict[str, Any]]:
        """ë‹¤ë‹¨ê³„ Fallbackì„ ì ìš©í•œ ìœ ì—°í•œ ì²­í‚¹"""
        try:
            processed_chunks = []
            
            # 1ë‹¨ê³„: ê·œì¹™ ë²ˆí˜¸ ê¸°ë°˜ ë¶„í•  ì‹œë„
            try:
                rule_chunks = self._try_rule_based_chunking(text)
                if len(rule_chunks) > 0:
                    logger.info("âœ… ê·œì¹™ ê¸°ë°˜ ì²­í‚¹ ì„±ê³µ")
                    return rule_chunks
            except Exception as e:
                logger.warning(f"âš ï¸ ê·œì¹™ ê¸°ë°˜ ì²­í‚¹ ì‹¤íŒ¨: {e}")
            
            # 2ë‹¨ê³„: ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  (Fallback #1)
            try:
                semantic_chunks = self._try_semantic_chunking(text)
                if len(semantic_chunks) > 0:
                    logger.info("âœ… ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì„±ê³µ")
                    return semantic_chunks
            except Exception as e:
                logger.warning(f"âš ï¸ ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ ì‹¤íŒ¨: {e}")
            
            # 3ë‹¨ê³„: ê¸°ë³¸ ë¶„í•  (Final Fallback)
            logger.warning("ğŸ”„ ê¸°ë³¸ ì²­í‚¹ìœ¼ë¡œ í´ë°±")
            return self._basic_chunking(text, pdf_filename)
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë“  ì²­í‚¹ ë°©ì‹ ì‹¤íŒ¨: {e}")
            return []

    def _try_rule_based_chunking(self, text: str) -> List[Dict[str, Any]]:
        """ê·œì¹™ ë²ˆí˜¸ ê¸°ì¤€ ë¶„í•  (ì œNí•­, N. í˜•íƒœ)"""
        
        # ë” ìœ ì—°í•œ íŒ¨í„´ë“¤
        patterns = [
            r'ì œ(\d+)í•­[.\s]*([^ì œ]+?)(?=ì œ\d+í•­|\Z)',  # ì œNí•­ íŒ¨í„´
            r'(\d+)\.\s*([^0-9]+?)(?=\d+\.|\Z)',        # 1. 2. 3. í˜•íƒœ
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            if len(matches) > 5:  # ì¶©ë¶„í•œ ê·œì¹™ì´ ìˆìœ¼ë©´ ì„±ê³µ
                return self._build_rule_chunks(matches, pattern)
        
        raise ValueError("ê·œì¹™ íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    def _build_rule_chunks(self, matches: List, pattern: str) -> List[Dict[str, Any]]:
        """ê·œì¹™ ë§¤ì¹­ ê²°ê³¼ë¥¼ ì²­í¬ë¡œ ë³€í™˜"""
        chunks = []
        
        for i, match in enumerate(matches):
            rule_number, content = match
            
            # ë‚´ìš© ì •ë¦¬
            clean_content = content.strip()
            if len(clean_content) < 10:  # ë„ˆë¬´ ì§§ì€ ë‚´ìš© ì œì™¸
                continue
            
            # ì´ˆë“±í•™ìƒìš© ì£¼ì œ ì¶”ì¶œ
            topics = self._extract_elementary_topics(clean_content)
            examples = self._extract_examples(clean_content)
            
            chunks.append({
                "id": f"korean_grammar_rule_{rule_number}_{i}",  # ê³ ìœ  ì¸ë±ìŠ¤ ì¶”ê°€
                "text": f"ì œ{rule_number}í•­: {clean_content}",
                "metadata": {
                    "source": "korean_grammar_official.pdf",
                    "document_type": "official_grammar_rule",
                    "rule_number": int(rule_number),
                    "sub_index": i,  # í•˜ìœ„ ì²­í¬ ì¸ë±ìŠ¤
                    "authority": "ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€",
                    "year": "2017",
                    "topics": ", ".join(topics) if topics else "",  # ë¦¬ìŠ¤íŠ¸ â†’ ë¬¸ìì—´
                    "examples": ", ".join(examples) if examples else "",  # ë¦¬ìŠ¤íŠ¸ â†’ ë¬¸ìì—´
                    "difficulty_level": "elementary",  # ì´ˆë“±í•™ìƒìš©
                    "chunk_method": "rule_based"
                }
            })
        
        logger.info(f"âœ… ê·œì¹™ ê¸°ë°˜ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ")
        return chunks

    def _try_semantic_chunking(self, text: str) -> List[Dict[str, Any]]:
        """ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  (ì´ˆë“±í•™ìƒì´ ìì£¼ í—·ê°ˆë¦¬ëŠ” ì£¼ì œë³„)"""
        
        # ì´ˆë“±í•™ìƒì´ ìì£¼ í—·ê°ˆë¦¬ëŠ” ë¬¸ë²• í‚¤ì›Œë“œ
        elementary_keywords = [
            "ëœì†Œë¦¬", "ììŒ", "ëª¨ìŒ", "ì™¸ë˜ì–´", "ë„ì–´ì“°ê¸°", 
            "ë°œìŒ", "í‘œê¸°", "í•œììŒ", "ë°›ì¹¨", "ì–´ê°„",
            "ë˜ë‹¤", "ë¼ë‹¤", "í•˜ë‹¤", "í•´ë‹¤"
        ]
        
        chunks = []
        current_chunk = ""
        current_topic = "ì¼ë°˜"
        
        lines = text.split('\n')
        for line in lines:
            # ìƒˆë¡œìš´ ì£¼ì œ ë°œê²¬
            detected_topic = self._detect_topic(line, elementary_keywords)
            
            if detected_topic != current_topic and current_chunk.strip():
                # ì´ì „ ì²­í¬ ì €ì¥
                chunks.append(self._create_semantic_chunk(
                    current_chunk, current_topic, len(chunks)
                ))
                current_chunk = line
                current_topic = detected_topic
            else:
                current_chunk += "\n" + line
        
        # ë§ˆì§€ë§‰ ì²­í¬
        if current_chunk.strip():
            chunks.append(self._create_semantic_chunk(
                current_chunk, current_topic, len(chunks)
            ))
        
        if len(chunks) < 3:
            raise ValueError("ì˜ë¯¸ì  ë¶„í•  ì‹¤íŒ¨")
        
        return chunks

    def _basic_chunking(self, text: str, pdf_filename: str) -> List[Dict[str, Any]]:
        """ê¸°ë³¸ ë¶„í•  (RecursiveCharacterTextSplitter)"""
        
        chunks = self.text_splitter.split_text(text)
        
        processed_chunks = []
        for i, chunk in enumerate(chunks):
            # ìµœì†Œí•œì˜ ë©”íƒ€ë°ì´í„°ë¼ë„ ì¶”ì¶œ
            topics = self._extract_elementary_topics(chunk)
            examples = self._extract_examples(chunk)
            
            processed_chunks.append({
                "id": f"korean_grammar_basic_{i}",
                "text": chunk.strip(),
                "metadata": {
                    "source": "korean_grammar_official.pdf",
                    "document_type": "official_grammar_rule",
                    "chunking_method": "basic_fallback",
                    "chunk_index": i,
                    "topics": ", ".join(topics) if topics else "",  # ë¦¬ìŠ¤íŠ¸ â†’ ë¬¸ìì—´
                    "examples": ", ".join(examples) if examples else "",  # ë¦¬ìŠ¤íŠ¸ â†’ ë¬¸ìì—´
                    "authority": "ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€",
                    "year": "2017",
                    "difficulty_level": "elementary"
                }
            })
        
        logger.info(f"âœ… ê¸°ë³¸ ì²­í‚¹ ì™„ë£Œ: {len(processed_chunks)}ê°œ")
        return processed_chunks

    def _detect_topic(self, text: str, keywords: List[str]) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ë²• ì£¼ì œ ê°ì§€"""
        text_lower = text.lower()
        
        for keyword in keywords:
            if keyword in text_lower:
                return keyword
        
        return "ì¼ë°˜"

    def _extract_elementary_topics(self, content: str) -> List[str]:
        """ì´ˆë“±í•™ìƒìš© ë¬¸ë²• ì£¼ì œ ì¶”ì¶œ"""
        topics = []
        
        # ì´ˆë“±í•™ìƒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì£¼ì œ ë¶„ë¥˜
        topic_keywords = {
            "ë„ì–´ì“°ê¸°": ["ë„ì–´ì“°ê¸°", "ë„ì–´ ì“°ê¸°", "ë„ìš°ê¸°"],
            "ë°›ì¹¨": ["ë°›ì¹¨", "ììŒ", "ã„±", "ã„´", "ã„·", "ã„¹", "ã…"],
            "ëª¨ìŒ": ["ëª¨ìŒ", "ã…", "ã…‘", "ã…“", "ã…•", "ã…—", "ã…›"],
            "ë°œìŒ": ["ë°œìŒ", "ì†Œë¦¬", "ì½ê¸°"],
            "ì™¸ë˜ì–´": ["ì™¸ë˜ì–´", "ì™¸êµ­ì–´", "ì˜ì–´"],
            "ì–´ë ¤ìš´_êµ¬ë¶„": ["ë˜ë‹¤", "ë¼ë‹¤", "í•˜ë‹¤", "í•´ë‹¤"]
        }
        
        for topic, keywords in topic_keywords.items():
            if any(keyword in content for keyword in keywords):
                topics.append(topic)
        
        return topics[:3]  # ìµœëŒ€ 3ê°œë§Œ

    def _extract_examples(self, content: str) -> List[str]:
        """ì˜ˆì‹œ ì¶”ì¶œ (ì´ˆë“±í•™ìƒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ê²ƒë“¤)"""
        examples = []
        
        # ì˜ˆì‹œ íŒ¨í„´ë“¤
        example_patterns = [
            r'[ê°€-í£]+/[ê°€-í£]+',      # ë‹¨ì–´1/ë‹¨ì–´2 í˜•íƒœ
            r'â—‹\s*[ê°€-í£\s]+',         # â—‹ ì˜¬ë°”ë¥¸ ì˜ˆì‹œ
            r'Ã—\s*[ê°€-í£\s]+',         # Ã— í‹€ë¦° ì˜ˆì‹œ
            r'ì˜ˆ:\s*[ê°€-í£\s,]+',      # ì˜ˆ: ë‹¨ì–´ë“¤
        ]
        
        for pattern in example_patterns:
            matches = re.findall(pattern, content)
            examples.extend(matches)
        
        # ë„ˆë¬´ ê¸´ ì˜ˆì‹œëŠ” ì œì™¸ (ì´ˆë“±í•™ìƒìš©)
        clean_examples = [ex.strip() for ex in examples if len(ex.strip()) < 20]
        
        return clean_examples[:5]  # ìµœëŒ€ 5ê°œë§Œ

    def _create_semantic_chunk(self, content: str, topic: str, index: int) -> Dict[str, Any]:
        """ì˜ë¯¸ ê¸°ë°˜ ì²­í¬ ìƒì„±"""
        return {
            "id": f"korean_grammar_semantic_{topic}_{index}",
            "text": content.strip(),
            "metadata": {
                "source": "korean_grammar_official.pdf",
                "document_type": "official_grammar_rule", 
                "chunking_method": "semantic",
                "topic": topic,
                "chunk_index": index,
                "authority": "ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€",
                "year": "2017",
                "difficulty_level": "elementary"
            }
        }


# ì „ì—­ PDF ë¡œë”
pdf_loader = None

def get_pdf_loader() -> PDFDataLoader:
    """ì „ì—­ PDF ë¡œë” ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global pdf_loader
    if pdf_loader is None:
        pdf_loader = PDFDataLoader()
    return pdf_loader

def load_pdf_documents(pdf_directory: str = "app/data/pdfs/") -> List[Dict[str, Any]]:
    """PDF ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  PDF ë¬¸ì„œ ë¡œë“œ"""
    loader = get_pdf_loader()
    all_documents = []
    
    if not os.path.exists(pdf_directory):
        logger.warning(f"PDF ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {pdf_directory}")
        return []
    
    # íŠ¹ì • íŒŒì¼ë§Œ ì²˜ë¦¬ (korea_grammar_official.pdf)
    target_file = "korea_grammar_official.pdf"
    pdf_path = os.path.join(pdf_directory, target_file)
    
    if not os.path.exists(pdf_path):
        logger.error(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        return []
    
    logger.info(f"ğŸ“„ PDF ì²˜ë¦¬ ì¤‘: {target_file}")
    
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    full_text = loader.extract_text_from_pdf(pdf_path)
    if full_text:
        # ì²­í‚¹
        chunks = loader.chunk_pdf_text(full_text, target_file)
        all_documents.extend(chunks)
        
        logger.info(f"âœ… {target_file} ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
    else:
        logger.error(f"âŒ {target_file} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
    
    logger.info(f"âœ… ì´ {len(all_documents)}ê°œ PDF ë¬¸ì„œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
    return all_documents